---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Machine Learning graduate student at the University of Warsaw advised by [Piotr Miłoś](https://scholar.google.com/citations?user=Se67XecAAAAJ&hl=en). 
I'm interested in many aspects of LLMs, including: long-context, efficiency, code generation, better reasoning capabilities. Previously Student Researcher at Google Research, mentored by [Yuhuai Wu](http://www.cs.toronto.edu/~ywu) and [Christian Szegedy](https://scholar.google.com/citations?user=bnQMuzgAAAAJ&hl=en).
I am also grateful to [Henryk Michalewski](https://www.mimuw.edu.pl/~henrykm/resume.html) and [Łukasz Kaiser](https://scholar.google.com/citations?user=JWmiQR0AAAAJ&hl=en) for supervising my [bachelor thesis](https://aclanthology.org/2022.findings-naacl.117.pdf).

My recent work, [Focused Transformer](https://arxiv.org/abs/2307.03170) ([LongLLaMA](https://github.com/CStanKonrad/long_llama)), develops a method for extending context length of existing LLMs like LLaMA. I also published papers on using language models for automated theorem proving (formal mathematics). I'm broadly interested in improving the capabilities of LLMs, be it long-context or better reasoning (math/coding).

I'm now on the industrial job market. This is my [CV](https://students.mimuw.edu.pl/~st406386/cv_st_23072.pdf). Please don't hesitate to drop me a message about any career opportunities! I'm particularly willing to relocate to the US/UK/Switzerland.


Publication
------
[**Focused Transformer: Contrastive Training for Context Scaling**](https://arxiv.org/abs/2307.03170)
**Szymon Tworkowski**, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś

[arXiv'23](https://arxiv.org/abs/2307.03170)

[**Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers**](https://arxiv.org/abs/2205.10893)
Albert Q. Jiang, Wenda Li, **Szymon Tworkowski**, Konrad Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, Mateja Jamnik

[NeurIPS 2022](https://openreview.net/forum?id=fUeOyt-2EOp)

[**Hierarchical Transformers Are More Efficient Language Models**](https://arxiv.org/abs/2110.13711)
Piotr Nawrot*, **Szymon Tworkowski***, Michał Tyrolski, Łukasz Kaiser, Yuhuai Wu, Christian Szegedy, Henryk Michalewski

[NAACL 2022, Findings](https://aclanthology.org/2022.findings-naacl.117.pdf)

[**Formal Premise Selection With Language Models**](http://aitp-conference.org/2022/abstract/AITP_2022_paper_32.pdf)
**Szymon Tworkowski***, Maciej Mikuła*, Tomasz Odrzygóźdź*, Konrad Czechowski*, Szymon Antoniak*, Albert Q. Jiang, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, Yuhuai Wu

[AITP 2022](http://aitp-conference.org/2022/abstract/AITP_2022_paper_32.pdf)

[**Magnushammer: A Transformer-based Approach to Premise Selection**](https://arxiv.org/abs/2303.04488)
Maciej Mikuła*, Szymon Antoniak*, **Szymon Tworkowski***, Albert Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, Yuhuai Wu

[arXiv'23](https://arxiv.org/abs/2303.04488)

[**Explaining Competitive-Level Programming Solutions using LLMs**](https://arxiv.org/abs/2307.05337)
Jierui Li, **Szymon Tworkowski**, Yingying Wu, Raymond Mooney

[ACL2023 NLRSE workshop](https://arxiv.org/abs/2307.05337)

Invited talks
------
[AITP 2022 - Formal Premise Selection With Language Models (recording on YT)](https://www.youtube.com/watch?v=gem5xO3FhQc)
