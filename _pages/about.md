---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


PhD student at the University of Warsaw advised by [Piotr Miłoś](https://scholar.google.com/citations?user=Se68XecAAAAJ&hl=en).
I'm interested in many aspects of LLMs, including: long context, math reasoning, code generation, LLM efficiency. Previously Student Researcher at Google, very fortunate to be mentored by [Yuhuai Wu](http://www.cs.toronto.edu/~ywu) and [Christian Szegedy](https://scholar.google.com/citations?user=bnQMuzgAAAAJ&hl=en).
I am also grateful to [Henryk Michalewski](https://www.mimuw.edu.pl/~henrykm/resume.html) and [Łukasz Kaiser](https://scholar.google.com/citations?user=JWmiQR0AAAAJ&hl=en) for supervising my [bachelor thesis](https://aclanthology.org/2022.findings-naacl.117.pdf).

My recent work, [Focused Transformer](https://arxiv.org/abs/2307.03170) ([LongLLaMA](https://github.com/CStanKonrad/long_llama)), develops a method for extending context length of existing LLMs like LLaMA, in an efficient way. I also published [papers](https://arxiv.org/abs/2205.10893) on using language models for automated theorem proving (formal mathematics).

My dream goal is to build more and more autonomous large language models, capable of assisting humans in solving difficult research-level problems, and ultimately even generating new scientific knowledge automatically.

I am very excited about learning foreign languages, most recently Mandarin - 叫我世梦就行。  

[//]: # (I'm now on the industrial job market. This is my [CV]&#40;https://students.mimuw.edu.pl/~st406386/cv_st_23072.pdf&#41;. Please don't hesitate to drop me a message about any career opportunities! I'm particularly willing to relocate to the US/UK/Switzerland.)


Publication
------
[**Focused Transformer: Contrastive Training for Context Scaling**](https://arxiv.org/abs/2307.03170)
**Szymon Tworkowski**, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś

[NeurIPS 2023](https://arxiv.org/abs/2307.03170)

[**Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers**](https://arxiv.org/abs/2205.10893)
Albert Q. Jiang, Wenda Li, **Szymon Tworkowski**, Konrad Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, Mateja Jamnik

[NeurIPS 2022](https://openreview.net/forum?id=fUeOyt-2EOp)

[**Hierarchical Transformers Are More Efficient Language Models**](https://arxiv.org/abs/2110.13711)
Piotr Nawrot*, **Szymon Tworkowski***, Michał Tyrolski, Łukasz Kaiser, Yuhuai Wu, Christian Szegedy, Henryk Michalewski

[NAACL 2022, Findings](https://aclanthology.org/2022.findings-naacl.117.pdf)

[**Formal Premise Selection With Language Models**](http://aitp-conference.org/2022/abstract/AITP_2022_paper_32.pdf)
**Szymon Tworkowski***, Maciej Mikuła*, Tomasz Odrzygóźdź*, Konrad Czechowski*, Szymon Antoniak*, Albert Q. Jiang, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, Yuhuai Wu

[AITP 2022](http://aitp-conference.org/2022/abstract/AITP_2022_paper_32.pdf)

[**Magnushammer: A Transformer-based Approach to Premise Selection**](https://arxiv.org/abs/2303.04488)
Maciej Mikuła*, Szymon Antoniak*, **Szymon Tworkowski***, Albert Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, Yuhuai Wu

[arXiv'23](https://arxiv.org/abs/2303.04488)

[**Explaining Competitive-Level Programming Solutions using LLMs**](https://arxiv.org/abs/2307.05337)
Jierui Li, **Szymon Tworkowski**, Yingying Wu, Raymond Mooney

[ACL2023 NLRSE workshop](https://arxiv.org/abs/2307.05337)

Invited talks
------
* **Google DeepMind, Zürich** - How to Make LLMs Utilize Long Context Efficiently? 
*Oct 9, 2023*

* **University of Edinburgh**, Edinburgh NLP Meeting - How to Make LLMs Utilize Long Context Efficiently?
*Oct 2, 2023*

* [ACL 2023 - Explaining Competitive-Level Programming Solutions using LLMs - interview with Letitia from AI Coffee Break](https://youtu.be/-Agcr0nawuk?t=87)

* [AITP 2022 - Formal Premise Selection With Language Models (recording on YT)](https://www.youtube.com/watch?v=gem5xO3FhQc)
